# coding=utf-8
# 搜索简书上python相关文档
import time
import hackhttp
from bs4 import BeautifulSoup as BS
import re
import threading
import Queue
start = time.time()
threads = []
url = []
raw='''
POST /search/do?q=python&type=note&page=&order_by=default HTTP/1.1
Accept: application/json
X-CSRF-Token: 7WivVZhdzmb+fv3JAfYB0PgGR6R0OCcWQ0V1wNyNvlsUbOuD/8BRPr75Wjo8GAWt6apkaDhuVutZRUvjDJAtfg==
Referer: https://www.jianshu.com/search?utf8=%E2%9C%93&q=python
Accept-Language: zh-CN
Accept-Encoding: gzip, deflate
User-Agent: Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko
Host: www.jianshu.com
Content-Length: 0
Connection: close
Cache-Control: no-cache
Cookie: signin_redirect=https%3A%2F%2Fwww.jianshu.com%2Fsearch%3Fq%3Dpython%26page%3D1%26type%3Dnote; read_mode=day; default_font=font2; locale=zh-CN; _m7e_session=a894ddd920949132cb1fc79b33073238; sensorsdata2015jssdkcross=%7B%22distinct_id%22%3A%22166c7a112305b3-091221ce98126e-192a72-921600-166c7a112328d1%22%2C%22%24device_id%22%3A%22166c7a112305b3-091221ce98126e-192a72-921600-166c7a112328d1%22%2C%22props%22%3A%7B%22%24latest_traffic_source_type%22%3A%22%E7%9B%B4%E6%8E%A5%E6%B5%81%E9%87%8F%22%2C%22%24latest_referrer%22%3A%22%22%2C%22%24latest_referrer_host%22%3A%22%22%2C%22%24latest_search_keyword%22%3A%22%E6%9C%AA%E5%8F%96%E5%88%B0%E5%80%BC_%E7%9B%B4%E6%8E%A5%E6%89%93%E5%BC%80%22%7D%7D; sajssdk_2015_cross_new_user=1; Hm_lvt_0c0e9d9b1e7d617b3e6842e85b9fb068=1540947514; Hm_lpvt_0c0e9d9b1e7d617b3e6842e85b9fb068=1540947514'''
f=open("F:\haha.txt",'w')
for i in range(1, 11):
    website = 'https://www.jianshu.com/search/do?q=python&type=note&page=' + str(i) + '&order_by=default'
    url.append(website)
que=Queue.Queue()
out_queue=Queue.Queue()
class threadurl(threading.Thread):
    def __init__(self,threadname,que):
        threading.Thread.__init__(self)
        self.threadname=threadname
        self.que=que
    def run(self):
        print "start:"+self.threadname
        while True:
            try:
                crawler(self.name,self.que)
            except:
                print 'error'
        # print "exiting" +self.name
def crawler(threadname,que):
    time.sleep(2)
    host=que.get(timeout=2)
    hh = hackhttp.hackhttp()
    code, head, html, redirect_url, log = hh.http(url=host, raw=raw)
    soup = BS(html, 'lxml')
    # print str(soup)
    # print soup.prettify()
    lianjie = re.compile('"title"(.*?)"content"').findall(str(soup))
    for i in lianjie:
        title = 'title' + i.split(',')[0].replace('<em class="search-result-highlight">', '').replace('</em>', '')
        href = 'https://www.jianshu.com/p/' + i.split(',')[1].split(':')[1].replace('"', '')
        result1 = title + ' _ ' + href + '\n'
        print result1
        f.write(result1)
threadList = ['thread-1','thread-2','thread-3','thread-4','thread-5']
for i in url:
    que.put(i)
for tname in  threadList:
    thread=threadurl(tname,que)
    thread.start()
    threads.append(thread)
for t in  threads:
    t.join()
shijan=time.time()-start
print "total time is:" + str(shijan)




